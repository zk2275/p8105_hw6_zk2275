---
title: "P8105_hw6_zk2275"
author: "Zhuodiao Kuang"
date: "`r Sys.Date()`"
output:
  github_document:
    toc: TRUE
    toc_depth: 2
---

<!------------------------------------------------------------------------------
Preamble
------------------------------------------------------------------------------->
```{r, packages loading and default set,echo = FALSE, message = FALSE, warning = FALSE}
# load necessary packages
library(tidyverse)
library(dplyr)
library(readxl)
library(rvest)
library(kableExtra)
library(modelr)
library(glmnet)
# set knitr defaults
knitr::opts_chunk$set(
               echo      = TRUE,
	             cache     = TRUE,
               prompt    = FALSE,
               tidy      = FALSE,
               comment   = NA,
               message   = FALSE,
               warning   = FALSE,
               dpi       = 150,
               fig.height= 6,
               fig.align = "center")
# set theme defaults
theme_set(
  theme_bw() +
  theme(
    legend.position = "bottom"
    , plot.title    = element_text(hjust = 0.5)
    , plot.subtitle = element_text(hjust = 0.5)    
    , plot.caption  = element_text(hjust = 0.0)
  )
)

# set color scale defaults
options(
    ggplot2.continuous.colour = "gradient"
  , ggplot2.continuous.fill   = "gradient"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete   = scale_fill_viridis_d
```
<!------------------------------------------------------------------------------
Problem 1
------------------------------------------------------------------------------->

# Problem 1



<!------------------------------------------------------------------------------
Problem 2
------------------------------------------------------------------------------->

# Problem 2

We now switch to Central Park weather data. Our goal is to estimate the parameters related to the linear relationship between the maximum and minimum daily temperatures through a bootstrapping procedure.

```{r}
# pull weather data from rnoaa
weather_df <-
  rnoaa::meteo_pull_monitors(
      c("USW00094728")
    , var      = c("PRCP", "TMIN", "TMAX")
    , date_min = "2017-01-01"
    , date_max = "2017-12-31"
  ) |>
  mutate(
      name = recode(id, USW00094728 = "CentralPark_NY")
    , tmin = tmin / 10
    , tmax = tmax / 10
  ) |>
  select(name, id, everything())

```

## Bootstrap

We consider the SLR as follows:

$$\text{temp}_t^{\text{max}} = \beta_0 + \beta_1 \text{temp}_t^{\text{min}} + \beta_2 \text{prcp}_t + \varepsilon_t$$

for each day $t$ in 2017. We create 5,000 resampled replicates of our weather data and run the above regression for each one.

```{r}
# set a seed for replicability
set.seed(8105)

# perform the bootstrap with 5000 replicates
bootstrap_output <-
  weather_df |>
  # execute bootstrap
  modelr::bootstrap(n = 5000) %>% 
  # for all replicate samples, store regression output via list column
  mutate(
      models         = map(strap, ~lm(tmax ~ tmin+prcp, data = .x) )
    , results_tidy   = map(models, broom::tidy)
    , results_glance = map(models, broom::glance)
  ) |> 
  # remove extraneous columns
  select(-strap, -models)
```


## Estimating the Coefficient of Determination

First, letâ€™s try to estimate the distribution of the estimator for the coefficient of determination, $R^2$.

```{r}

# unnest result for r2
r2_estimate <-
  bootstrap_output %>%
  # select just the broom::glance output to get r2
  select(-results_tidy) %>%
  # unnest result
  unnest(results_glance) %>%
  # keep relevant columns
  select(.id, r.squared)

# create a density plot of the distribution of our estimates
r2_estimate |>
  # instantiate plot
  ggplot(aes(x = r.squared)) +
  # add density
  geom_density() +
  # add meta-data
  labs(
    title = "Distribution of Estimates of Coefficient of Determination"
    , x = ""
    , y = "Density"
  )

```



The 95% confidence interval around our estimate of $R^2$ is given by [`r round(quantile(pull(r2_estimate, r.squared), 0.05 / 2), 4)`, `r round(quantile(pull(r2_estimate, r.squared), 1 - 0.05 / 2), 4)`]. The distribution has a very slight left skew, but it largely seems to follow a normal distribution with mean `r round(mean(pull(r2_estimate, r.squared)), 4)` and standard deviation of `r round(sd(pull(r2_estimate, r.squared)), 4)`.

## Estimating a Non-linear Function of the Regression Coefficients

Next, let's try to estimate the distribution of the estimator for $S \equiv \log(\beta_1 \times \beta_2)$.


```{r distribution_logb1b2}
# unnest result for coefficients and compute log(b1*b2)
log_prod_estimate <-
  bootstrap_output |>
  # select just the broom::tidy output to get b1 and b2
  select(-results_glance) |>
  # unnest result
  unnest(results_tidy) |>
  # keep relevant columns
  select(.id, term, estimate) |>
  # prepare results for pivot
  mutate(term = recode(
      term,
     "tmin"        = "b1",
     "prcp"        = "b2"
  )) |>
  # pivot wider to enable row-wise computation
  pivot_wider(
      names_from  = term
    , values_from = estimate
  ) |>
  # compute estimate as function of b1 and b2
  mutate(log_prod = log(b1 * b2),
         abs_log_prod = log(abs(b1 * b2)),
         b1b2 = b1 * b2)
```

We notice that $b1 \times b2$ are always negative:

```{r}
log_prod_estimate|>
  pull(b1b2) |>
summary()
```

So we plot $log(|b1 \times b2|)$ instead.


```{r}
# create a density plot of the distribution of our estimates
log_prod_estimate |>
  # instantiate plot
  ggplot(aes(x = abs_log_prod)) +
  # add density
  geom_density() +
  # add meta-data
  labs(
    title = "Distribution of Absolute Estimates of S"
    , x = ""
    , y = "Density"
  )
```


The 95% confidence interval around our estimate of $\log(|\beta_1 \times \beta_2|)$ is given by [`r round(quantile(pull(log_prod_estimate, abs_log_prod), 0.05 / 2), 4)`, `r round(quantile(pull(log_prod_estimate, abs_log_prod), 1 - 0.05 / 2), 4)`]. Once again, the distribution has a very slight left skew, but it largely seems to follow a normal distribution with mean `r round(mean(pull(log_prod_estimate, abs_log_prod)), 4)` and standard deviation of `r round(sd(pull(log_prod_estimate, abs_log_prod)), 4)`.

<!------------------------------------------------------------------------------
Problem 3
------------------------------------------------------------------------------->

# Problem 3

## Preliminary Analysis

By using data on mothers, fathers, and the infants themselves, we hope to find potential predictors of infant birth weight. We import our dataset below and factorize variables as needed.

```{r}
# construct tidied dataset of birth weight and potential predictors
birthweight <-
  # pull in csv
  read_csv("birthweight.csv") |>
  # factorize variables where appropriate
  mutate(
    # recode babysex as a binary factor variable
    babysex = factor(recode(
      babysex
    , `1` = "Male"
    , `2` = "Female"
    ))
    # recode mrace as a factor variable
  , mrace = factor(recode(
      mrace
    , `1` = "White"
    , `2` = "Black"
    , `3` = "Asian"
    , `4` = "Puerto Rican"
    , `8` = "Other"
    ))
    # recode frace as a factor variable, noting it has an additional level
  , frace = factor(recode(
      frace
    , `1` = "White"
    , `2` = "Black"
    , `3` = "Asian"
    , `4` = "Puerto Rican"
    , `8` = "Other"
    , `9` = "Unknown"
    ))
    # recode malform as a binary factor variable
  , malform = factor(recode(
      malform
    , `0` = "Absent"
    , `1` = "Present"
    ))
    )

# create parents' races for further analysis
birthweight <- birthweight |>
  mutate( prace = factor(
      ifelse((mrace == "Black" & frace == "Black"), "Black",
      ifelse((mrace == "White" & frace == "White"), "White", "Other")
            )
                        )
      )

```

Note that, the dataset contains no missing values.

```{r}
# show that all cell entries are non-missing
all(!is.na(birthweight))
```

Cleaning is therefore an easy task. Having said that, we narrow down and eliminate some of the predictors that might not be relevant for our analysis in the sections that follow. Regression analysis may not always be appropriate even when all of the data are clean.

### Predictor: Race of Parents

We have created the variable `prace`, which can indicate one of three things for a given observation: (1) both parents are black, (2) both parents are white, or (3) the parents are any other combination of races. We do this because scenarios (1) and (2) together capture about `r round(100 * mean(pull(birthweight, prace) != "Other"), 0)`% of observations. As such, the variable `prace` is in line with the principle of parsimony, and hence we will employ `prace` in place of `mrace` and `frace`.

### Predictor: Incidence of Prior Problematic Pregnancy

We first observe that the variables `pnumlbw` and `pnumsga` consist solely of zeroes. As they offer no variation, they are not suitable for prediction in this dataset, so we will omit them from our analysis.

```{r}
# show that pnumlbw is always zero
all(pull(birthweight, pnumlbw) == 0)

# show that pnumsga is always zero
all(pull(birthweight, pnumsga) == 0)
```

### Predictor: Incidence of Malformations

Intuitively, the `malform` variable, which indicates the presence of any malformations that might impact birth weight, seems like an excellent candidate for a predictor of birth weight! Unfortunately, in our dataset, only `r sum(pull(birthweight, malform) == "Present")` observations indicate the presence of such a malformation. While this lack of variation is not as extreme as that of `pnumlbw` or `pnumsga`, we will still opt to omit this candidate predictor from our analysis.

### Predictor: Change in Weight

By construction, we have that `wtgain` $\equiv$ `delwt` - `ppwt`. That is, a mother's weight gain during pregnancy is tautologically the difference between her weight at delivery and her weight prior to pregnancy. We show this is true empirically, as well, just as a sanity check.

```{r}
# show that wtgain = delwt - ppwt tautologically
all(
  birthweight |>
  # define test variable that returns TRUE if wtgain = delwt - ppwt
  mutate(wtgain_compare = wtgain == (delwt - ppwt)) |>
  # pull test variable to be evaluated by the all function
  pull(wtgain_compare)
)
```

As such, including `wtgain`, `delwt`, and `ppwt` simultaneously as predictors would introduce perfect multicollinearity into our model. In particular, if $X$ is our data matrix, $X^tX$ would have no inverse, and hence our projection matrix would be undefined. Fortunately, even if we had not noticed this linear dependence, the `lm` function would have been clever enough to drop one of the three predictors. However, we _have_ noticed this issue, and hence we will omit `wtgain` from our analysis. The choice of which variable to omit is arbitrary, of course, but we prefer to retain the variables that were measured directly.

### Predictor: Family Income

The `fincome` variable is a bit tricky. Should we consider the logarithm of `fincome`? Values denote monthly income in hundreds, rounded. Moreover, values seem to be top-coded at 96, such that any family with a household salary exceeding \$114,600 is grouped into the same category. Because of this arbitrary cap, `fincome` does not seem log-normal, as shown below. (Note: As $\log(0) = -\infty$, we omit the single observation for which `fincome` = 0.)

```{r distribution_fincome}
# ploot histogram of log(income)
birthweight |>
  # omit single row for which fincome = 0
  filter(fincome > 0) |>
  # instantiate plot
  ggplot(aes(x = log(100 * 12 * fincome))) +
  # add histogram
  geom_histogram() +
  # add meta-data
  labs(
      title = "Distribution of Annual Family Income (Logged)"
    , x     = "Log(Income)"
    , y     = "Frequency"
  )
```

Should `fincome` be a factor variable? There are 12 levels of `fincome`, so even if none of the levels truly predict `bwt`, there is a `r round(100 * (1 - (1 - 0.05)^12), 0)`% chance that at least one level would wrongly appear as significant at significance level $\alpha$ = 0.05. As such, it does not seem that factorizing `fincome` is appropriate, either. After all of this deliberation, we conclude it is best to simply leave `fincome` as is - a continuous random variable recorded discretely.


## Model Selection

While we have eliminated variables that are obviously bad choices for predictors of birth weight, we will turn to machine learning to eliminate those that are less obviously bad. In particular, we invoke the `glmnet` function to conduct a lasso regression. Below, we use `model.matrix` to convert factor variables to dummy variables, which we can then feed to `glmnet` to obtain the near-optimal choice of $\lambda$, the shrinkage penalty.

```{r optimal_shrinkage_penalty}
# identify output vector y
y <- pull(birthweight, bwt)

# set up input matrix x
x <- birthweight %>% select(
    # output
    bwt
    # baby inputs
  , babysex, bhead, blength
    # mom inputs
  , delwt, gaweeks, menarche, mheight, momage, parity, ppbmi, ppwt, smoken
    # family inputs
  , fincome, prace
  )

# convert factor variables to dummy variables
x <- model.matrix(~ ., dplyr::select(x, -bwt))[, -1]

# set a seed since results can vary slightly from iteration to iteration
set.seed(8105)

# conduct lasso for various choices of lambda
cv_model <- cv.glmnet(x, y, alpha = 1)
plot(cv_model)
```

Per the algorithm's recommendation, we see that `lambda.1se` (the value of $\lambda$ corresponding to the center vertical dashed line) yields an MSE very close to that of `lambda.min`  (the value of $\lambda$ corresponding to the left vertical dashed line). As such, we can choose a parsimonious model without losing much predictive power. Below are seven variables that survive the second-stage culling.

```{r}
# create data frame of the selected variables after lasso
coef(cv_model, s = cv_model[["lambda.1se"]]) 
```

We finally have our model! All that's left is to run the regression and compare our residuals to our fitted values.

```{r}
# fit final model
final_model <- lm(
    # ouput
    bwt
    # baby inputs post-lasso
  ~ bhead + blength
    # mom inputs post-lasso
  + delwt + gaweeks + mheight + smoken
    # family inputs post-lasso
  + prace
  , data = birthweight
)

# output statistics for estimators
final_model %>%
  summary() %>%
  broom::tidy() %>%
  knitr::kable()

# output statistics for overall model
final_model %>%
  summary() %>%
  broom::glance() %>%
  knitr::kable()
```

All of our variables appear as highly significant ($p$ < 0.01), and our adjusted $R^2 \approx$ 71.4%. Given our parsimonious selection of variables, this seems like a good fit! Let's compare our model residuals to our model predictions to validate the results.

```{r residuals_vs_predictions}
# plot scatter plot of residuals against fitted values
birthweight %>%
  # add predicted values
  add_predictions(final_model) %>%
  # add residuals
  add_residuals(final_model) %>%
  # instantiate plot
  ggplot(aes(x = pred, y = resid)) +
  # add points
  geom_point(alpha = 0.1, color = "blue") +
  # add smoothed lines
  geom_smooth(se = FALSE, color = "orange") + 
  # deepen line y = 0 for reference
  geom_hline(yintercept = 0, color = "black") + 
  # add meta-data
  labs(
    title = "Residuals against Predicted Values"
    , x = "Predicted Values"
    , y = "Residuals"
  )
```

There are two observations for which a negative birth weight is predicted, but this is not unreasonable for an unconstrained linear regression. Though the residuals tend to skew positive for fitted values below 1,750, only about `r round(100 * sum((birthweight %>% add_predictions(final_model) %>% pull(pred)) < 1750) / nrow(birthweight), 2)`% of observations lie below this threshold. Otherwise, residuals seem to generally be dispersed evenly about the line $y = 0$. That is, outside the aforementioned outliers, the errors seem to be mean-zero and homoscedastic.


## Model Comparison

Finally, we compare our model to two other linear model candidates. The first model considers length at birth and gestational age in weeks as inputs. The second model considers head circumference at birth, length at birth, and sex of the child - as well as all possible interactions - as inputs.

```{r model_comparison}
# create alt model with inputs blength and gaweeks
model_alt1 <- lm(bwt ~ blength + gaweeks, data = birthweight)

# create alt model with baby variables and all orders of interactions
model_alt2 <- lm(bwt ~ (bhead + blength + babysex)^3, data = birthweight)

# set seed since crossv_mc randomly assigns rows to the training dataset
set.seed(8105)

# create 100 training/testing partitions of the data
cv_split <-
  # use the default 80/20 split for each partition
  crossv_mc(birthweight, 100) %>% 
  # convert lists to tibbles
  mutate(
      train = map(train, as_tibble)
    , test  = map(test,  as_tibble)
  )

# construct data frame of rmses across models and across training/testing splits
rmse_df <-
  cv_split %>%
  # fit models to training data
  mutate(
      final = map(.x = train, ~ final_model)
    , alt1  = map(.x = train, ~ model_alt1)
    , alt2  = map(.x = train, ~ model_alt2)
  ) %>%
  # apply fitted modls to testing data for validation
  mutate(
    rmse_final = map2_dbl(.x = final, .y = test, ~rmse(model = .x, data = .y)),
    rmse_alt1  = map2_dbl(.x = alt1,  .y = test, ~rmse(model = .x, data = .y)),
    rmse_alt2  = map2_dbl(.x = alt2,  .y = test, ~rmse(model = .x, data = .y))
  ) %>%
  # retain only necessary columns
  select(rmse_final, rmse_alt1, rmse_alt2) %>%
  # pivot longer
  pivot_longer(
      cols         = everything()
    , names_to     = "model"
    , values_to    = "rmse"
    , names_prefix = "rmse_"
  ) %>%
  # factorize models to preserve ordering
  mutate(model = fct_inorder(model))

# create violin plot of distribution of rmse by model
rmse_df %>%
  # instantiate plot
  ggplot(aes(x = model, y = rmse, fill = model)) +
  # add violin chart
  geom_violin(alpha = 0.5) +
  # suppress legend
  theme(legend.position = "none") +
  # add meta-data
  labs(
      title = "Distribution of RMSE, by Model"
    , x     = "Model"
    , y     = "RMSE"
  ) +
  # add x-axis labels
  scale_x_discrete(labels = c(
      "My Model"
    , "Alternate Model I"
    , "Alternate Model II"
  ))
```

Our lasso-inspired model seems to generally perform the best, though the model with interaction terms also seems to do well. Conversely, the distribution of prediction errors for the model with only two inputs lies entirely above that of our model, so we can confidently say that our model is strictly better than this alternative. Moreover, in addition to performing better on average, our model is much easier to interpret than the model with interactions, as the effect of each input in the latter model depends on the other two inputs. 

